[
  {
    "objectID": "4.genome-annotation.html",
    "href": "4.genome-annotation.html",
    "title": "Genome Annotation",
    "section": "",
    "text": "Objectives\n\n\nAssessing genome quality\nAssigning taxonomy\nPredicting gene sequences\n\n\n\n\n\nTo assess the quality of the assembled genomes, we will use CheckM2, which assesses the degree of completess and contamination of microbial genomes (whether they are from isolates, single cell or metagenome data) using machine learning.\nFirst, let’s gather all assembled genomes into one directory.\nmkdir -p 7.genome_annotation/all_assembled_genomes\n\nfor i in {01..10}; do\n  cp 6.assembly_evaluation/v3/scaffolds_\"$i\"_v3.m1000.fasta 7.genome_annotation/all_assembled_genomes/genome_\"$i\".fasta\ndone\n1. Create a directory for CheckM2 files\nmkdir 7.genome_annotation/genome_quality\n2. Run CheckM2\nCreate SLURM script to run CheckM2.\nnano checkm2.sh\nand copy the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      checkm2\n#SBATCH --time          00:30:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_checkm2_%j.err\n#SBATCH --output        slurm_checkm2_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load Python\nmodule load CheckM2/1.0.1-Miniconda3\n\nsrun checkm2 predict --threads 10  -x fasta --input 7.genome_annotation/all_assembled_genomes/ --output-directory 7.genome_annotation/genome_quality/\nSubmit SLURM job.\nsbatch checkm2.sh\nOutput files\nCheckM2 generates several output files and directories that provide detailed metrics on genome quality.\npredicted_quality.tsv: this table contains the estimated completeness and contamination for each genome, along with other quality metrics. Key columns include:\n\nCompleteness: Percentage of expected single-copy genes present in the genome. High completeness (&gt;90%) indicates a near-complete genome.\nContamination: Percentage of duplicated single-copy genes, indicating contamination or strain heterogeneity. Low contamination (&lt;5%) is ideal.\nStrain Heterogeneity: A measure of strain-level variation. High values may indicate the presence of multiple closely related strains.\n\n3. Create a subset of CheckM2 results for later use (visualisation)\ncd 7.genome_annotation/genome_quality\n\n# Define input and output files\ninput_file=\"quality_report.tsv\"\noutput_file=\"genome_quality.txt\"\n\n# Process the file\nawk 'BEGIN {OFS=\"\\t\"} \n     NR==1 {print \"ID\", $2, $3, $9; next} \n     {print $1, $2, $3, $12}' \"$input_file\" &gt; \"$output_file\"\n\n# Display the result\ncat \"$output_file\"\n\n\n\n\nTo assign taxonomy to the assembled genomes, we are using GTDB-Tk (Genome Taxonomy Database Toolkit), a bioinformatics tool designed for the taxonomic classification of bacterial and archaeal genomes. It uses the Genome Taxonomy Database (GTDB), which provides a standardized taxonomy based on genome phylogeny based on a set of conserved single-copy proteins.\n1. Create a directory for GTDB-Tk files\nmkdir 7.genome_annotation/taxonomy\n2. Run GTDB-Tk\nCreate SLURM script\nnano taxonomy.sh\nand copy the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      gtdbtk\n#SBATCH --time          02:00:00\n#SBATCH --mem           140GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_gtdbtk_%j.err\n#SBATCH --output        slurm_gtdbtk_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load GTDB-Tk/2.4.0-foss-2023a-Python-3.11.6\n\nsrun gtdbtk classify_wf \\\n  -x fasta \\\n  --cpus 10 \\\n  --genome_dir 7.genome_annotation/all_assembled_genomes/ \\\n  --skip_ani_screen \\\n  --out_dir 7.genome_annotation/taxonomy/\nSubmit SLURM job.\nsbatch taxonomy.sh\nOutput files\nGTDB-Tk generates several output files, including:\nclassification_summary.tsv: a summary of taxonomic classifications, including the most specific taxonomic rank achieved (e.g., species or genus). Key columns include:\n\nuser_genome: input genome filename.\nclassification: full taxonomic classification in GTDB format (e.g., d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; …).\nfastani_reference: GTDB reference genome with the highest average nucleotide identity (ANI).\nclassification_method: method used for classification, typically based on ANI or phylogenetic placement. gtdbtk_out/classify/gtdbtk.bac120.summary.tsv: Detailed classification information for bacterial genomes, including ANI values and confidence scores. gtdbtk_out/classify/gtdbtk.ar122.summary.tsv: Detailed classification information for archaeal genomes.\n\n\n\n\n\nHere, we aim to identify protein-coding sequences, translate them into protein sequences, and generate functional annotations for each gene.\nThis step uses Prokka, a tool specifically desgined to find and annotate gene coding sequences and other genomic features from prokaryotes and viruses. It first uses Prodigal which is a very popular and robust protein-coding gene prediction tool that can handle draft genomes, then uses a variety of databases when trying to assign function to the predicted gene sequences.\n1. Create a directory for gene prediction\nCreate a new directory for predicted genes files.\nmkdir 7.genome_annotation/gene_prediction\n2. Run Prokka\nWrite a SLURM script to automate Prokka for processing all assembled genomes.\nnano gene_prediction.sh\nCopy the following script in gene_prediction.sh\n#!/bin/bash -e\n\n#SBATCH --account       uoa00626\n#SBATCH --job-name      prokka\n#SBATCH --time          00:15:00\n#SBATCH --mem           1GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_prokka_%j.err\n#SBATCH --output        slurm_prokka_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load prokka/1.14.5-GCC-9.2.0\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\n    prokka 7.genome_annotation/all_assembled_genomes/genome_${array[$SLURM_ARRAY_TASK_ID]}.fasta \\\n            --outdir 7.genome_annotation/gene_prediction \\\n            --prefix genome_${array[$SLURM_ARRAY_TASK_ID]} \\\n            --cpus 10 \\\nSubmit SLURM job.\nsbatch gene_prediction.sh\nOutput files\nProkka generates an array of new files per genome:\nls 7.genome_annotation/gene_prediction\n    genome_01.err\n    genome_01.faa\n    genome_01.ffn\n    genome_01.fna\n    genome_01.fsa\n    genome_01.gbk\n    genome_01.gff\n    genome_01.log\n    genome_01.sqn\n    genome_01.tbl\n    genome_01.tsv\n    genome_01.txt\n    ...\n\n\n\n\n\n\n\nExtension\nDescription\n\n\n\n\n.gff\nThis is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV.\n\n\n.gbk\nThis is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-Genbank, with one record for each sequence.\n\n\n.fna\nNucleotide FASTA file of the input contig sequences.\n\n\n.faa\nProtein FASTA file of the translated CDS sequences.\n\n\n.ffn\nNucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)\n\n\n.sqn\nAn ASN1 format “Sequin” file for submission to Genbank. It needs to be edited to set the correct taxonomy, authors, related publication etc.\n\n\n.fsa\nNucleotide FASTA file of the input contig sequences, used by “tbl2asn” to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.\n\n\n.tbl\nFeature Table file, used by “tbl2asn” to create the .sqn file.\n\n\n.err\nUnacceptable annotations - the NCBI discrepancy report.\n\n\n.log\nContains all the output that Prokka produced during its run. This is a record of what settings you used, even if the –quiet option was enabled.\n\n\n.txt\nStatistics relating to the annotated features found.\n\n\n.tsv\nTab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product",
    "crumbs": [
      "Day Two",
      "Genome Annotation"
    ]
  },
  {
    "objectID": "4.genome-annotation.html#genome-annotation",
    "href": "4.genome-annotation.html#genome-annotation",
    "title": "Genome Annotation",
    "section": "",
    "text": "Objectives\n\n\nAssessing genome quality\nAssigning taxonomy\nPredicting gene sequences\n\n\n\n\n\nTo assess the quality of the assembled genomes, we will use CheckM2, which assesses the degree of completess and contamination of microbial genomes (whether they are from isolates, single cell or metagenome data) using machine learning.\nFirst, let’s gather all assembled genomes into one directory.\nmkdir -p 7.genome_annotation/all_assembled_genomes\n\nfor i in {01..10}; do\n  cp 6.assembly_evaluation/v3/scaffolds_\"$i\"_v3.m1000.fasta 7.genome_annotation/all_assembled_genomes/genome_\"$i\".fasta\ndone\n1. Create a directory for CheckM2 files\nmkdir 7.genome_annotation/genome_quality\n2. Run CheckM2\nCreate SLURM script to run CheckM2.\nnano checkm2.sh\nand copy the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      checkm2\n#SBATCH --time          00:30:00\n#SBATCH --mem           50GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_checkm2_%j.err\n#SBATCH --output        slurm_checkm2_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load Python\nmodule load CheckM2/1.0.1-Miniconda3\n\nsrun checkm2 predict --threads 10  -x fasta --input 7.genome_annotation/all_assembled_genomes/ --output-directory 7.genome_annotation/genome_quality/\nSubmit SLURM job.\nsbatch checkm2.sh\nOutput files\nCheckM2 generates several output files and directories that provide detailed metrics on genome quality.\npredicted_quality.tsv: this table contains the estimated completeness and contamination for each genome, along with other quality metrics. Key columns include:\n\nCompleteness: Percentage of expected single-copy genes present in the genome. High completeness (&gt;90%) indicates a near-complete genome.\nContamination: Percentage of duplicated single-copy genes, indicating contamination or strain heterogeneity. Low contamination (&lt;5%) is ideal.\nStrain Heterogeneity: A measure of strain-level variation. High values may indicate the presence of multiple closely related strains.\n\n3. Create a subset of CheckM2 results for later use (visualisation)\ncd 7.genome_annotation/genome_quality\n\n# Define input and output files\ninput_file=\"quality_report.tsv\"\noutput_file=\"genome_quality.txt\"\n\n# Process the file\nawk 'BEGIN {OFS=\"\\t\"} \n     NR==1 {print \"ID\", $2, $3, $9; next} \n     {print $1, $2, $3, $12}' \"$input_file\" &gt; \"$output_file\"\n\n# Display the result\ncat \"$output_file\"\n\n\n\n\nTo assign taxonomy to the assembled genomes, we are using GTDB-Tk (Genome Taxonomy Database Toolkit), a bioinformatics tool designed for the taxonomic classification of bacterial and archaeal genomes. It uses the Genome Taxonomy Database (GTDB), which provides a standardized taxonomy based on genome phylogeny based on a set of conserved single-copy proteins.\n1. Create a directory for GTDB-Tk files\nmkdir 7.genome_annotation/taxonomy\n2. Run GTDB-Tk\nCreate SLURM script\nnano taxonomy.sh\nand copy the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      gtdbtk\n#SBATCH --time          02:00:00\n#SBATCH --mem           140GB\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_gtdbtk_%j.err\n#SBATCH --output        slurm_gtdbtk_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load GTDB-Tk/2.4.0-foss-2023a-Python-3.11.6\n\nsrun gtdbtk classify_wf \\\n  -x fasta \\\n  --cpus 10 \\\n  --genome_dir 7.genome_annotation/all_assembled_genomes/ \\\n  --skip_ani_screen \\\n  --out_dir 7.genome_annotation/taxonomy/\nSubmit SLURM job.\nsbatch taxonomy.sh\nOutput files\nGTDB-Tk generates several output files, including:\nclassification_summary.tsv: a summary of taxonomic classifications, including the most specific taxonomic rank achieved (e.g., species or genus). Key columns include:\n\nuser_genome: input genome filename.\nclassification: full taxonomic classification in GTDB format (e.g., d__Bacteria; p__Proteobacteria; c__Gammaproteobacteria; …).\nfastani_reference: GTDB reference genome with the highest average nucleotide identity (ANI).\nclassification_method: method used for classification, typically based on ANI or phylogenetic placement. gtdbtk_out/classify/gtdbtk.bac120.summary.tsv: Detailed classification information for bacterial genomes, including ANI values and confidence scores. gtdbtk_out/classify/gtdbtk.ar122.summary.tsv: Detailed classification information for archaeal genomes.\n\n\n\n\n\nHere, we aim to identify protein-coding sequences, translate them into protein sequences, and generate functional annotations for each gene.\nThis step uses Prokka, a tool specifically desgined to find and annotate gene coding sequences and other genomic features from prokaryotes and viruses. It first uses Prodigal which is a very popular and robust protein-coding gene prediction tool that can handle draft genomes, then uses a variety of databases when trying to assign function to the predicted gene sequences.\n1. Create a directory for gene prediction\nCreate a new directory for predicted genes files.\nmkdir 7.genome_annotation/gene_prediction\n2. Run Prokka\nWrite a SLURM script to automate Prokka for processing all assembled genomes.\nnano gene_prediction.sh\nCopy the following script in gene_prediction.sh\n#!/bin/bash -e\n\n#SBATCH --account       uoa00626\n#SBATCH --job-name      prokka\n#SBATCH --time          00:15:00\n#SBATCH --mem           1GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_prokka_%j.err\n#SBATCH --output        slurm_prokka_%j.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load prokka/1.14.5-GCC-9.2.0\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\n    prokka 7.genome_annotation/all_assembled_genomes/genome_${array[$SLURM_ARRAY_TASK_ID]}.fasta \\\n            --outdir 7.genome_annotation/gene_prediction \\\n            --prefix genome_${array[$SLURM_ARRAY_TASK_ID]} \\\n            --cpus 10 \\\nSubmit SLURM job.\nsbatch gene_prediction.sh\nOutput files\nProkka generates an array of new files per genome:\nls 7.genome_annotation/gene_prediction\n    genome_01.err\n    genome_01.faa\n    genome_01.ffn\n    genome_01.fna\n    genome_01.fsa\n    genome_01.gbk\n    genome_01.gff\n    genome_01.log\n    genome_01.sqn\n    genome_01.tbl\n    genome_01.tsv\n    genome_01.txt\n    ...\n\n\n\n\n\n\n\nExtension\nDescription\n\n\n\n\n.gff\nThis is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV.\n\n\n.gbk\nThis is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-Genbank, with one record for each sequence.\n\n\n.fna\nNucleotide FASTA file of the input contig sequences.\n\n\n.faa\nProtein FASTA file of the translated CDS sequences.\n\n\n.ffn\nNucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)\n\n\n.sqn\nAn ASN1 format “Sequin” file for submission to Genbank. It needs to be edited to set the correct taxonomy, authors, related publication etc.\n\n\n.fsa\nNucleotide FASTA file of the input contig sequences, used by “tbl2asn” to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.\n\n\n.tbl\nFeature Table file, used by “tbl2asn” to create the .sqn file.\n\n\n.err\nUnacceptable annotations - the NCBI discrepancy report.\n\n\n.log\nContains all the output that Prokka produced during its run. This is a record of what settings you used, even if the –quiet option was enabled.\n\n\n.txt\nStatistics relating to the annotated features found.\n\n\n.tsv\nTab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product",
    "crumbs": [
      "Day Two",
      "Genome Annotation"
    ]
  },
  {
    "objectID": "1.introduction.html",
    "href": "1.introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This workshop aims to provide a comprehensive understanding of microbial genome assembly using short-read sequencing data. It showcases the different stages involved in assembling bacterial genomes from raw sequencing data of bacterial isolates, including quality control, genome assembly using a de novo approach, as well as genome annotation and visualisation.\nThis workshop assumes that attendees have basic knowledge in commandline and R scripting.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1.introduction.html#sequencing-and-genome-assembly",
    "href": "1.introduction.html#sequencing-and-genome-assembly",
    "title": "Introduction",
    "section": "Sequencing and genome assembly",
    "text": "Sequencing and genome assembly\nThe majority of microbial genome assemblies today are based on short-read sequencing platforms such as Illumina, which offer the advantages of high throughput, low cost, and relatively low error rates. However, short reads present challenges in genome assembly due to their limited length and the presence of repetitive regions in genomes. While longer reads (e.g., from PacBio or Oxford Nanopore) can provide better coverage of these regions, short-read sequencing remains the most common and accessible method for routine microbial genome assembly.\n\nDe novo vs. reference-guided assembly\nDe novo genome assembly refers to the process of constructing a genome from scratch without using a reference genome. This approach is particularly useful when working with organisms for which no high quality, closed reference genome is available (which also is quite the norm for microorganisms). The process of de novo assembly relies on the identification of overlap regions on the sequence reads themselves to reconstruct genomes. It is often preferred for microbial genomes when high-quality, non-contaminated sequencing data is available, as it avoids biases that may arise from comparison to reference genomes that would be, for example, phylogenetically too distant.\nIn contrast, reference-guided assembly involves mapping short-read sequences to an existing reference genome of a closely related species. This method relies on the reference genome to guide the assembly process, making it faster and potentially more accurate, particularly for well-studied organisms.\n\n\nGenome assembly workflow\n\n\n\nKey considerations\nThere are a few general points that are worth considering when setting up a study involving sequencing and genome assembly from microbial isolates, with emphasis on each point likely influenced by the research question(s):\n\nSequencing read length: “short” read length usually sits between 50 and 300 bp, longer reads can improve assembly contiguity and genome quality.\nSequencing depth: number of times each base is read during sequencing. Higher depth reduces sequencing errors and noise, leading to more accurate base calls.\nContamination: non-target DNA (e.g., from host or environment) can interfere with the assembly process.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1.introduction.html#a-bit-of-background-on-the-data",
    "href": "1.introduction.html#a-bit-of-background-on-the-data",
    "title": "Introduction",
    "section": "A bit of background on the data",
    "text": "A bit of background on the data\nData used in this workshop was retrieved from Tee et al., 2021: “Genome streamlining, plasticity, and metabolic versatility distinguish co-occurring toxic and nontoxic cyanobacterial strains of Microcoleus” (https://doi.org/10.1128/mBio.02235-21).\nThis workshop works through sequencing data from 10 isolates of Microcoleus. Species from this benthic cyanobacterial are ubiquitous and form thick mats in freshwater systems, such as rivers, that are sometimes toxic due to the production of potent neurotoxins (anatoxins).",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1.introduction.html#attribution-notice",
    "href": "1.introduction.html#attribution-notice",
    "title": "Introduction",
    "section": "Attribution notice",
    "text": "Attribution notice\nThis workshop was created using materials from BIOSCI701 by Dr. Kim Handley at the University of Auckland.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "2.sequencing-read-qc.html",
    "href": "2.sequencing-read-qc.html",
    "title": "Sequencing Read Quality Control",
    "section": "",
    "text": "Objective: When receiving fresh sequencing data, there are a few critical steps to ensure that sequences are good enough to be assembled into genomes. Poor quality sequencing data can lead to failed genome assemblies or misleading results. The main objective of this section is to assess sequencing data quality, and get the sequences cleaned up and ready for genome assembly. This step ensures that unwanted artifacts like sequencing adapters or low-quality sequences are removed before assembly.\n\nKey considerations in sequence quality control:\n\nSequence quality/PHRED score: a measure of the confidence in each base call; typically, a PHRED score &gt;30 is considered high quality.\nSequencing adapters: short artificial sequences used during library preparation that must be removed to avoid errors in genome assembly.\nRead length distribution: checking for unusually short or degraded reads that may indicate sequencing issues.\nGC content bias: deviations from expected GC content may suggest contamination or sequencing bias.\n\n\n\n1. Initial quality control\nThe first step in quality control is to assess the overall quality of your raw sequencing data. For that, we use a popular tool called FastQC, which generates a comprehensive quality report for each FASTQ file.\n\n1. Verify Raw Sequencing Files\nRun the following command to confirm all expected FASTQ files are present.\nls 0.raw_reads/\nThere should be 20 FASTQ files in 0.raw_reads/\n    raw.01_R1.fastq\n    raw.01_R2.fastq\n    raw.02_R1.fastq\n    raw.02_R2.fastq\n    raw.03_R1.fastq\n    raw.03_R2.fastq\n    raw.04_R1.fastq\n    raw.04_R2.fastq\n    raw.05_R1.fastq\n    raw.05_R2.fastq\n    raw.06_R1.fastq\n    raw.06_R2.fastq\n    raw.07_R1.fastq\n    raw.07_R2.fastq\n    raw.08_R1.fastq\n    raw.08_R2.fastq\n    raw.09_R1.fastq\n    raw.09_R2.fastq\n    raw.10_R1.fastq\n    raw.10_R2.fastq\n\nEnsure forward (_R1) and reverse (_R2) reads are paired correctly.\nUse clear file-naming conventions for consistency.\n\n\n2. Run FastQC\nWe will now look at the overall quality of the raw sequences with FastQC. Clear out previous modules to avoid conflicts and load FastQC.\nmodule purge &gt;/dev/null 2&gt;&1\n\nmodule load FastQC/0.12.1\nRun FastQC on the raw sequencing reads.\nfastqc 0.raw_reads/*.fastq -o 1.raw_reads_QC/\nAn update on FastQC process will automatically appear as follows:\n    Started analysis of raw.01_R1.fastq\n    Approx 5% complete for raw.01_R1.fastq\n    Approx 10% complete for raw.01_R1.fastq\n    Approx 15% complete for raw.01_R1.fastq\n    Approx 20% complete for raw.01_R1.fastq\n    Approx 25% complete for raw.01_R1.fastq\n    ...\n\n3. Review FastQC Output\nLet’s check what files were generated by FastQC.\ncd 1.raw_reads_QC/\n\nls\n    raw.01_R1_fastqc.html\n    raw.01_R1_fastqc.zip\n    raw.01_R2_fastqc.html\n    raw.01_R2_fastqc.zip\n    raw.02_R1_fastqc.html\n    raw.02_R1_fastqc.zip\n    raw.02_R2_fastqc.html\n    raw.02_R2_fastqc.zip\n    ...\nGenerated files include .html files (quality reports viewable in a web browser) and .zip files (detailed raw data as text files).\nNow let’s have a look at one of the FastQC .html files, and inspect key metrics:\n\nGeneral statistics\n\n\n\n\nFastQC - General statistics\n\n\n\nPer Base Quality Scores: High-quality sequences should have median scores above 28.\n\n\n\n\nFastQC - Per Base Quality Scores\n\n\n\nAdapter content: High levels indicate adapter contamination that must be removed.\n\n\n\n\nFastQC - Adapter content\n\n\n\nPer Sequence GC Content: Deviations may suggest contamination.\n\nLook for consistent quality across samples and identify problematic samples requiring special attention (i.e. red/yellow flags).\n\n4. Summarize results with MultiQC\nMultiQC compiles multiple FastQC output files into an interactive report for easier interpretation, which is useful when you have an important number of samples\nFirst, load MutliQC\nmodule load MultiQC/1.24.1-foss-2023a-Python-3.11.6\nMove into the directory with the FastQC results and run MultiQC\ncd 1.raw_reads_QC/\n\nmultiqc .\nNow let’s have a look at the MultiQC report: \nQuestions\n\nAre there any sequencing parameters that differ between samples?\nWhat sequencing adapters were used?\n\n\n\n\n2. Adapter trimming and quality filtering\nOnce you have assessed the raw data quality, it’s time to trim sequencing adapters and filter out low-quality reads. We will use BBMap’s BBduk tool for trimming, and FastQC combined with MultiQC again to generate quality reports for the clean reads.\n\n1. Run BBduk for trimming and filtering\nnano read_trimming.sh\nCopy in read_trimming.sh\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      read_trimming\n#SBATCH --time          0:05:00\n#SBATCH --mem           4GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 10\n#SBATCH --error         slurm_read_trimming_%A-%a.err\n#SBATCH --output        slurm_read_trimming_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load BBMap/39.01-GCC-11.3.0\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\n#Remove sequencing adapters:\nbbduk.sh in1=0.raw_reads/raw.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq in2=0.raw_reads/raw.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n out1=2.clean_reads/ad.trim.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq out2=2.clean_reads/ad.trim.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n ref=/opt/nesi/mahuika/BBMap/39.01-GCC-11.3.0/resources/adapters.fa \\\n hdist=1 ktrim=r ordered minlen=80 minlenfraction=0.33 mink=11 tbo tpe rcomp=f k=23\n\n#Quality trim reads:\nbbduk.sh in1=2.clean_reads/ad.trim.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq in2=2.clean_reads/ad.trim.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n out1=2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq out2=2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n outs=2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_single.fastq \\\n qtrim=rl trimq=30 minlen=80 \nSubmit SLURM job.\nsbatch read_trimming.sh\nExplanation of BBMap’s bbduk parameters\n\n\n\n\n\n\n\nAdapter trimming\n\n\n\n\n\nin1, in2\nInput paired-end read files\n\n\nout1, out2\nOutput trimmed paired-end read files\n\n\nref=/opt/nesi/mahuika/BBMap/39.01-GCC-11.3.0/resources/adapters.fa\nA reference file for adapter sequences to be removed. Use default adapter reference files unless working with a custom library\n\n\nhdist=1\nAllow one mismatch when identifying adapter sequences\n\n\nktirm=r\nTrim adapters using k-mer matching\n\n\nordered\nOutput reads in same order as input\n\n\nminlen=80\nDiscards any read shorter than 80 bp after trimming\n\n\nminlenfraction=033\nEnsures reads are at least 33% of their original length\n\n\nmink=11\nLook for shorter kmers at read tips down to this length, when k-trimming or masking\n\n\ntbo\nTrim adapters based on where paired reads overlap\n\n\ntpe\nWhen kmer right-trimming, trim both reads to the minimum length of either\n\n\nrcomp=f\nDon’t look for reverse-complements of kmers in addition to forward kmers\n\n\nk=23\nKmer length used for finding contaminants. Contaminants shorter than k will not be found. k must be at least 1\n\n\n\n\n\n\n\n\n\n\nQuality trimming\n\n\n\n\n\nin1, in2\nInput paired-end read files\n\n\nout1, out2\nOutput trimmed paired-end read files\n\n\nouts\nOutput trimmed single read file\n\n\nqtrim=rl trimq=30\nQuality trim reads (trim low-quality bases from both ends with a quality score threshold of 30)\n\n\nminlen=80\nKeeps reads with a minimum length of 80 after trimming\n\n\n\n\n2. Evaluate cleaned reads quality\nNow run FastQC again, then compile results with MultiQC.\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load FastQC/0.12.1\nmodule load MultiQC/1.24.1-foss-2023a-Python-3.11.6\n\nfastqc 2.clean_reads/clean.*.fastq -o 3.clean_reads_QC/\n\nmultiqc 3.clean_reads_QC/. -o 3.clean_reads_QC/\n\n\n\n3. Process evaluation\nCompare quality metrics pre- and post-trimming to ensure the data is ready for assembly, focusing on:\n\nTotal reads: ensure trimming didn’t excessively reduce the dataset.\nPer base quality: check for improved median quality.\nAdapter content: confirm adapters have been successfully removed.\n\nLet’s gather the information needed to evaluate changes in metrics listed above.\ncp 1.raw_reads_QC/multiqc_data/multiqc_fastqc.txt 4.read_trimming_evaluation/raw_multiqc_fastqc.txt\ncp 3.clean_reads_QC/multiqc_data/multiqc_fastqc.txt 4.read_trimming_evaluation/clean_multiqc_fastqc.txt\nRun the custom script below to extract info from MultiQC files and calculate how much of the data was retained post adapter trimming and quality filtering.\ncd 4.read_trimming_evaluation\n\nnano read_trimming_stats.sh\nCopy in read_trimming_stats.sh\n# Extract relevant columns (Sample and Total Sequences) from clean data\ncut -f1,5 clean_multiqc_fastqc.txt &gt; clean_reads_stats.txt\n\n# Remove 'clean.' from sample names\nsed -E 's/^clean\\.//' clean_reads_stats.txt &gt; clean_reads_stats_mod.txt\n\n# Extract relevant columns (Sample and Total Sequences) from raw data\ncut -f5 raw_multiqc_fastqc.txt &gt; raw_reads_stats.txt\n\n# Add row in raw read stats file for clean single read stats\n# Print the header\nhead -n 1 raw_reads_stats.txt &gt; raw_reads_stats_mod.txt\n\n# Skip the header, process the rest of the file, and duplicate each value three times\ntail -n +2 raw_reads_stats.txt | uniq | while read line; do\n    # Output the unique value three times\n    echo \"$line\" &gt;&gt; raw_reads_stats_mod.txt\n    echo \"$line\" &gt;&gt; raw_reads_stats_mod.txt\n    echo \"$line\" &gt;&gt; raw_reads_stats_mod.txt\ndone\n\n# Merge raw and clean data\npaste clean_reads_stats_mod.txt raw_reads_stats_mod.txt &gt; all_reads_stats.txt\n\n# Add header with specified column names and calculate percentage\nawk 'BEGIN {print \"Sample\\tClean reads\\tRaw reads\\tReads retained (%)\"} \n     NR&gt;1 {printf \"%s\\t%s\\t%s\\t%.2f%%\\n\", $1, $2, $3, ($2 / $3) * 100}' all_reads_stats.txt &gt; stats_output.txt\n\n# Clean up temporary files\nrm clean_reads_stats.txt clean_reads_stats_mod.txt raw_reads_stats.txt raw_reads_stats_mod.txt all_reads_stats.txt\nMake the script executable, then run it.\nchmod +x read_trimming_stats.sh\n\n./read_trimming_stats.sh\nLet’s have a look at the summary file we created (stats_output.txt).\nless stats_output.txt\nThe content should look something like this:\nSample  Clean reads     Raw reads       Reads retained (%)\n01_R1   472980.0        556289.0        85.02%\n01_R2   472980.0        556289.0        85.02%\n01_single       54427.0 556289.0        9.78%\n02_R1   832669.0        999116.0        83.34%\n02_R2   832669.0        999116.0        83.34%\n02_single       106322.0        999116.0        10.64%\nNow that we have clean and high quality short reads, we can proceed to genome assembly.",
    "crumbs": [
      "Day One",
      "Sequencing Read Quality Control"
    ]
  },
  {
    "objectID": "5b.analyses-visualisation.html",
    "href": "5b.analyses-visualisation.html",
    "title": "Phylogenetic tree visualisation",
    "section": "",
    "text": "Load libraries\nImport Newick tree file generated by FastTree and created a tree object\n\ntree &lt;- read.tree(\"~/Desktop/GA2025/01.Short_read_workshop/SR_workshop/temp/tree_file.txt\")\n\nNow we can visualize the phylogenetic tree with ggtree\n\nggtree(tree) + geom_tiplab()\n\n\n\n\n\n\n\n\nThis quick and easy tree doesn’t include a reference genome, so we can use the midpoint() function of the phangorn package to reroot the tree at midpoint (i.e. locate the midpoint of the longest path between any two tips and place the root at that location, assuming a constant evolutionary rate).\n\n#Rerooting tree at midpoint\nreroot_tree &lt;- midpoint(tree)\n\n# Rescale tree based on the maximum tree depth and plot a cleaned up version of the tree\ntree_depth &lt;- max(node.depth.edgelength(reroot_tree))\n\np0 &lt;-  ggtree(reroot_tree) + \n  geom_tiplab(align = TRUE, linesize = 0.5) +  \n  xlim(0, tree_depth * 1.2) +\n  theme_tree2()\np0\n\n\n\n\n\n\n\n\nLet’s now visualise some of the genome quality metrics that were gathered during the workshop. Read in the genome metadata.\n\nqual &lt;- read.delim(\"~/Desktop/GA2025/01.Short_read_workshop/SR_workshop/temp/genome_quality.txt\")\n\nJoin tree info with metadata.\n\n# Generate random values for each tip label in the data\nd1 &lt;- data.frame(id=reroot_tree$tip.label)\ncolnames(d1)[1] &lt;- \"ID\"\nd1 &lt;- left_join(d1, qual, \"ID\")\n\nFacet plot for completeness, contamination, and genome size.\n\n#Facet for tree\np1 &lt;- ggtree(reroot_tree)\n#Facet for genome completeness\np2 &lt;- facet_plot(p1, panel=\"Completeness (%)\", data=d1, geom=geom_point, aes(x=Completeness), color='red3')\n#Facet for genome contamination\np3 &lt;- facet_plot(p2, panel=\"Contamination (%)\", data=d1, geom=geom_point, aes(x=Contamination), color='green4')\n#Facet for genome size\np4 &lt;- facet_plot(p3, panel=\"Genome Size (bp)\", data=d1, geom=geom_segment, aes(x=0, xend=Genome_Size, y=y, yend=y), size=3, color='blue4')\n\n# Show all three plots with a scale\np4 + theme_tree2()",
    "crumbs": [
      "Day Two",
      "Phylogenetic tree visualisation"
    ]
  },
  {
    "objectID": "3.genome-assembly.html",
    "href": "3.genome-assembly.html",
    "title": "Genome Assembly",
    "section": "",
    "text": "Objective: Using quality filtered reads to assemble bacterial genomes, and assessing how different assembler parameters can impact assembly quality.\n\n\n1. De novo genome assembly with SPAdes\nSPAdes is a popular de novo assembler for microbial genome assembly. It uses a multi-kmer strategy, contains assembly pipelines for isolated prokaryotic genome data and supports Illumina short reads, single-cell sequencing, and hybrid assembly (Illumina + Nanopore/PacBio).\n\n1. Create slurm script\nnano spades_assembly_v1.sh\nCopy in spades_assembly_v1.sh\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_assembly_v1\n#SBATCH --time          00:45:00\n#SBATCH --mem           10GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 8\n#SBATCH --error         slurm_spades_assembly_v1_%A-%a.err\n#SBATCH --output        slurm_spades_assembly_v1_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load SPAdes/4.0.0-foss-2023a-Python-3.11.6\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\nmkdir -p 5.spades_assembly/v1/${array[$SLURM_ARRAY_TASK_ID]}\n\nsrun spades.py -k auto -t 8 \\\n  -1 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq \\\n  -2 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n  -s 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_single.fastq \\\n  -o 5.spades_assembly_v1/${array[$SLURM_ARRAY_TASK_ID]}/\nSubmit slurm job\nsbatch spades_assembly_v1.sh\n\n\n\nExplanation of SPAdes parameters\n\n\n\n\n\n-k\nK-mer sizes\n\n\n-t\nNumber of threads\n\n\n-1\nFile with forward reads\n\n\n-2\nFile with reverse reads\n\n\n-s\nFile with unpaired reads\n\n\n-o\nOutput directory\n\n\n\nOutput files\nSPAdes generates several important output files:\n\ncontigs.fasta: Contains the assembled contigs (contiguous sequences without gaps).\nscaffolds.fasta: Contains scaffolds, which are contigs linked together by paired-end information, with gaps represented as “N” bases. This file is useful for gene prediction and other genome-wide analyses.\nassembly_graph: Represents the de Bruijn graph used during assembly. This is useful for advanced users who want to investigate assembly ambiguities or potential misassemblies.\n\n\n\n\n2. Optimizing SPAdes\n\nObjective: Explore how different SPAdes parameters affect assembly quality and contiguity\n\nSPAdes provides several options to adjust and optimize the assembly process: - The k-mer size affects how reads overlap and form contigs. - The –careful mode (mismatch correction) may improve accuracy but reduce contiguity. - Adjusting coverage cutoffs can help remove low quality contigs.\n\n1. Assemble an isolate using different k-mer settings\nBy default, SPAdes automatically selects k-mer sizes, but we can manually specify them. The recommended k-mer sizes for different sequencing technologies can be found in the SPAdes documentation.\n–&gt; Repeat steps in 3.1 and change kmer sizes.\nnano spades_assembly_v2.sh\nCopy in spades_assembly_v2.sh\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_assembly_v2\n#SBATCH --time          00:45:00\n#SBATCH --mem           10GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 8\n#SBATCH --error         slurm_spades_assembly_v2_%A-%a.err\n#SBATCH --output        slurm_spades_assembly_v2_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load SPAdes/4.0.0-foss-2023a-Python-3.11.6\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\nmkdir -p 5.spades_assembly/v2/${array[$SLURM_ARRAY_TASK_ID]}\n\nsrun spades.py -k 33,55,77,99,121 -t 8 \\\n  -1 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq \\\n  -2 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n  -s 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_single.fastq \\\n  -o 5.spades_assembly/v2/${array[$SLURM_ARRAY_TASK_ID]}/\nSubmit slurm job\nsbatch spades_assembly_v2.sh\n\n2. Experiment with --careful\nThe –careful mode aims to reduce the number of mismatches and short indels. This option is recommended only for assembly of small genomes such as bacterial genomes, but not for larger genomes.\n–&gt; Repeat steps in 3.1 and add the –careful flag.\nnano spades_assembly_v3.sh\nCopy in 4.spades_assembly_v3.sh\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      spades_assembly_v3\n#SBATCH --time          00:45:00\n#SBATCH --mem           10GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 8\n#SBATCH --error         slurm_spades_assembly_v3_%A-%a.err\n#SBATCH --output        slurm_spades_assembly_v3_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1\nmodule load SPAdes/4.0.0-foss-2023a-Python-3.11.6\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\nmkdir -p 5.spades_assembly/v3/${array[$SLURM_ARRAY_TASK_ID]}\n\nsrun spades.py -k auto --careful -t 8 \\\n  -1 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R1.fastq \\\n  -2 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_R2.fastq \\\n  -s 2.clean_reads/clean.${array[$SLURM_ARRAY_TASK_ID]}_single.fastq \\\n  -o 5.spades_assembly/v3/${array[$SLURM_ARRAY_TASK_ID]}/\nSubmit slurm job\nsbatch spades_assembly_v3.sh\n\n\n\n3. Comparing assembly statistics\n1. Filter out short sequences\nBefore looking at the assemblies in more detail, we will first remove scaffolds that are shorter than 1000 bp. It is common practice to do so - there are several reasons why, such as minimizing assembly artefacts and improving downstream analyses such as gene predictions (that is partly because the average bacterial gene length is aroud 900 to 1000 bp).\nTo filter out sequences &lt; 1000 bp, we will use seqmagick. Clear out previous modules to avoid conflicts and load seqmagick.\nmodule purge\nmodule load seqmagick/0.8.4-gimkl-2020a-Python-3.8.2    \nRun seqmagick on all scaffold files using a loop.\nfor i in {01..10}; do\n  for j in v1 v2 v3; do\n    seqmagick convert --min-length 1000 5.spades_assembly/\"$j\"/\"$i\"/scaffolds.fasta 5.spades_assembly/\"$j\"/\"$i\"/scaffolds_\"$i\"_\"$j\".m1000.fasta\n  done\ndone\n\n2. Gather all assembled genomes into one folder\nfor i in {01..10}; do\n  for j in v1 v2 v3; do\n    mkdir -p 6.assembly_evaluation/\"$j\" | cp 5.spades_assembly/\"$j\"/\"$i\"/scaffolds_\"$i\"_\"$j\".m1000.fasta 6.assembly_evaluation/\"$j\"/scaffolds_\"$i\"_\"$j\".m1000.fasta\n  done\ndone\n\n3.Extract stats and evaluate assembly quality\nTo evaluate the quality of genome assemblies, we will use BBMap’s statswrapper.sh tool. This tool provides comprehensive assembly statistics, allowing us to assess key metrics crucial for determining assembly quality.\n\nNumber of contigs/scaffolds: Fewer contigs/scaffolds generally indicate a more contiguous assembly, which is desirable. However, an extremely low number might indicate chimeric assemblies or misassemblies.\nN50/L50: Length of the shortest contig at 50% of the total genome length. A higher N50 indicates more contiguous assemblies / Number of contigs representing 50% of the genome. A lower L50 is typically better, as it suggests fewer but longer contigs.\nTotal number of basepairs in assembly: This should roughly match the expected genome size. Significant deviations may suggest incomplete assembly or contamination.\nGC Content: Check for abnormal GC content, which could indicate contamination.\nMaximum scaffold/contig length: Provides an idea of the longest assembled sequence.\n\nNow let’s run BBMap’s statswrapper.sh on the different variations of assemblies perfomed using a loop.\nmodule purge\nmodule load BBMap/39.01-GCC-11.3.0 \nfor j in v1 v2 v3; do\n    statswrapper.sh 6.assembly_evaluation/\"$j\"/* &gt; 6.assembly_evaluation/assembly_stats_\"$j\".txt \ndone\nLet’s have a look at the assembly quality metrics assessed by BBMap statswrapper.sh tool.\ncd 6.assembly_evaluation/\n\nhead -n 3 assembly_stats_v1.txt\nn_scaffolds     n_contigs       scaf_bp contig_bp       gap_pct scaf_N50        scaf_L50        ctg_N50 ctg_L50 scaf_N90        scaf_L90        ctg_N90 ctg_L90 scaf_max        ctg_max scaf_n_gt50K scaf_pct_gt50K   gc_avg  gc_std  filename\n108     125     6451046 6449346 0.026   15      120978  19      100947  52      35705   62      31253   401116  377043  40      82.328  0.44562 0.01855 /scale_wlg_nobackup/filesets/nobackup/uoa00626/emilie_working/test2-SR-workshop/5.assembly_evaluation/v1/scaffolds_01_v1.m1000.fasta\n1049    1086    7872491 7870141 0.030   102     21818   109     20218   550     2148    572     2109    84541   84541   18      15.058  0.44444 0.02782 /scale_wlg_nobackup/filesets/nobackup/uoa00626/emilie_working/test2-SR-workshop/5.assembly_evaluation/v1/scaffolds_02_v1.m1000.fasta\nSubset files to view only the 3 key metrics mentioned above.\necho \"SPAdes assembly v1\" && cut -f1,3,7 assembly_stats_v1.txt \necho \"SPAdes assembly v2\" && cut -f1,3,7 assembly_stats_v2.txt \necho \"SPAdes assembly v3\" && cut -f1,3,7 assembly_stats_v3.txt \nDiscussion points:\n\nHow do N50 values differ between default vs. custom k-mer assembly?\nDoes manually setting k-mers improve contiguity?\nDoes --cov-cutoff auto remove unwanted small contigs?\nWhat are the trade-offs between accuracy (–careful) and contiguity (N50, fewer contigs)?",
    "crumbs": [
      "Day One",
      "Genome Assembly"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Microbial Genome Assembbly with Short Reads",
    "section": "",
    "text": "Welcome to our online workshop on Microbial Genome Assembly with Short Reads. This workshop runs over two half-days, from 9am - 1pm each day. This material will remain live and available, and assumes that you are working on the NeSI (New Zealand eScience Infrastructure) HPC system."
  },
  {
    "objectID": "5a.analyses-visualisation.html",
    "href": "5a.analyses-visualisation.html",
    "title": "Analyses Visualisation",
    "section": "",
    "text": "Objectives - Generating a genome map - Finding secondary metabolites related to toxin production - Building a phylogenetic tree\n\n\n\n\nGenoVi is a tool that generates customizable circular genome maps from draft or complete prokaryotic genomes. It is worth nothing that although we will not use that functionality in this workshop, GenoVi can also annotate and parse functional categories present in the genome and plot them onto the map.\n1. Create a new directory and fetch data\nWe will use GenoVi on a single genome (genome_03), using the corresponding GenBank formatted file that Prokka generated.\nmkdir -p 8.analyses/genome_map\n\ncp 7.genome_annotation/gene_prediction/genome_07.gbk 8.analyses/genome_map/.\n2. Run GenoVi in the terminal\ncd 8.analyses/genome_map/\n\nmodule purge\nmodule load GenoVi/0.2.16-Miniconda3\n\ngenovi -i genome_07_.gff -s draft -w 1000 -cu -cs paradise -bc white --size -te -t 'filename'\n\n\n\n\n\n\n\nExplanation of GenoVi parameters\n\n\n\n\n\n-i\ninput file (.gbk or .gff format)\n\n\n-s\ngenome status, i.e. “draft” or “complete”\n\n\n-w\nminimum sequence length in basepair to assign a GC analysis\n\n\n-cu\ndo not classify each coding sequence into Clusters of Orthologous Groups of proteins (COGs)\n\n\n-cs\ncolour scheme\n\n\n-bc\nbackground color\n\n\n–size\ndisplays the genome size value on each map\n\n\n–te\nadds text legend to the different tracks\n\n\n-t\nfigure title\n\n\n\nOutput files\nInside the genovi directory created, we can find: - PNG and SVG files of the genome map. - Gral_Stats.csv containing stats about number of CDSs, tRNA, etc per contig and total.\n\n\n\n\nSome of the genomes we are working with are from toxic strains of the cyanobacterial genus Microcoleus. They produce anatoxins, including anatoxin-a (otherwise known as Very Fast Death Factor), as secondary metabolites. We can find secondary metabolite Biosynthetic Gene Clusters (BGCs) using the annotation tool AntiSMASH either via the online server, or via the commandline.\n1. Create output directory and copy in predicted gene sequences files\nmkdir 8.analyses/BGC\n\ncp 7.genome_annotation/gene_prediction/*.genes.fna 8.analyses/BGC/.\n2. Run AntiSMASH\nOpen a new slurm script.\nnano secondary_metabolites.sh\nAnd copy in the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      antismash\n#SBATCH --time          0:30:00\n#SBATCH --mem           5GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 12\n#SBATCH --error         slurm_antismash_%A-%a.err\n#SBATCH --output        slurm_antismash_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1  \nmodule load antiSMASH/6.0.1-gimkl-2020a-Python-3.8.2\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\nsrun antismash --cb-subclusters --cb-knownclusters --cb-general --smcog-trees -c 12 --taxon bacteria --asf \\\n  --genefinding-tool prodigal \\\n  5.assembly_evaluation/all_assembled_genomes/scaffolds_${array[$SLURM_ARRAY_TASK_ID]}.m1000.fasta  \\\n  --output-dir 8.analyses/BGC/${array[$SLURM_ARRAY_TASK_ID]}\n\n\n\n\n\n\n\nExplanation of AntiSMASH parameters\n\n\n\n\n\n–cb-subclusters\ncompare identified clusters against known subclusters responsible for synthesising precursors\n\n\n–cb-knownclusters\ncompare identified clusters against known gene clusters from the MIBiG database\n\n\n–cb-general\ncompare identified clusters against a database of antiSMASH-predicted clusters\n\n\n–smcog-trees\ngenerate phylogenetic trees of sec. met. cluster orthologous groups.\n\n\n-c\nnumber of CPUs\n\n\n–taxon\ntaxonomic classification of input sequence\n\n\n–asf\nrun active site finder analysis\n\n\n–genefinding-tool\nspecify algorithm used for gene finding\n\n\n\nOutput files - index.html: web browser based summary - .gbk files are created for each biosynthetic gene cluster identified\n\n\n\n\nLet’s make a quick and easy phylogenetic tree with the 10 isolates, using the core gene alignment generated by GTDB-Tk and FastTree. FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences.\n1. Make a new directory\nmkdir -p 8.analyses/phylogenetic_tree\n2. Fetch the core gene alignment from GTDB-Tk\nUnzip and move over gtdbtk.bac120.user_msa.fasta.gz\ngzip -d 7.genome_annotation/taxonomy/align/gtdbtk.bac120.user_msa.fasta.gz &gt; 8.analyses/phylogenetic_tree/gtdbtk.bac120.user_msa.fasta\n\ncd 8.analyses/phylogenetic_tree/\n3. Build the phylogenetic tree\nIn the terminal, load the FastTree module then run it.\nmodule load FastTree/2.1.11-GCCcore-9.2.0\n\nFastTree gtdbtk.bac120.user_msa.fasta &gt; tree_file.txt\nWe will move to RStudio now to visualise the tree.",
    "crumbs": [
      "Day Two",
      "Analyses Visualisation"
    ]
  },
  {
    "objectID": "5a.analyses-visualisation.html#analyses-and-visualisation",
    "href": "5a.analyses-visualisation.html#analyses-and-visualisation",
    "title": "Analyses Visualisation",
    "section": "",
    "text": "Objectives - Generating a genome map - Finding secondary metabolites related to toxin production - Building a phylogenetic tree\n\n\n\n\nGenoVi is a tool that generates customizable circular genome maps from draft or complete prokaryotic genomes. It is worth nothing that although we will not use that functionality in this workshop, GenoVi can also annotate and parse functional categories present in the genome and plot them onto the map.\n1. Create a new directory and fetch data\nWe will use GenoVi on a single genome (genome_03), using the corresponding GenBank formatted file that Prokka generated.\nmkdir -p 8.analyses/genome_map\n\ncp 7.genome_annotation/gene_prediction/genome_07.gbk 8.analyses/genome_map/.\n2. Run GenoVi in the terminal\ncd 8.analyses/genome_map/\n\nmodule purge\nmodule load GenoVi/0.2.16-Miniconda3\n\ngenovi -i genome_07_.gff -s draft -w 1000 -cu -cs paradise -bc white --size -te -t 'filename'\n\n\n\n\n\n\n\nExplanation of GenoVi parameters\n\n\n\n\n\n-i\ninput file (.gbk or .gff format)\n\n\n-s\ngenome status, i.e. “draft” or “complete”\n\n\n-w\nminimum sequence length in basepair to assign a GC analysis\n\n\n-cu\ndo not classify each coding sequence into Clusters of Orthologous Groups of proteins (COGs)\n\n\n-cs\ncolour scheme\n\n\n-bc\nbackground color\n\n\n–size\ndisplays the genome size value on each map\n\n\n–te\nadds text legend to the different tracks\n\n\n-t\nfigure title\n\n\n\nOutput files\nInside the genovi directory created, we can find: - PNG and SVG files of the genome map. - Gral_Stats.csv containing stats about number of CDSs, tRNA, etc per contig and total.\n\n\n\n\nSome of the genomes we are working with are from toxic strains of the cyanobacterial genus Microcoleus. They produce anatoxins, including anatoxin-a (otherwise known as Very Fast Death Factor), as secondary metabolites. We can find secondary metabolite Biosynthetic Gene Clusters (BGCs) using the annotation tool AntiSMASH either via the online server, or via the commandline.\n1. Create output directory and copy in predicted gene sequences files\nmkdir 8.analyses/BGC\n\ncp 7.genome_annotation/gene_prediction/*.genes.fna 8.analyses/BGC/.\n2. Run AntiSMASH\nOpen a new slurm script.\nnano secondary_metabolites.sh\nAnd copy in the following:\n#!/bin/bash -e\n\n#SBATCH --account       nesi02659\n#SBATCH --job-name      antismash\n#SBATCH --time          0:30:00\n#SBATCH --mem           5GB\n#SBATCH --array         0-9\n#SBATCH --cpus-per-task 12\n#SBATCH --error         slurm_antismash_%A-%a.err\n#SBATCH --output        slurm_antismash_%A-%a.out\n\nmodule purge &gt;/dev/null 2&gt;&1  \nmodule load antiSMASH/6.0.1-gimkl-2020a-Python-3.8.2\n\ndeclare -a array=(\"01\" \"02\" \"03\" \"04\" \"05\" \"06\" \"07\" \"08\" \"09\" \"10\") \n\nsrun antismash --cb-subclusters --cb-knownclusters --cb-general --smcog-trees -c 12 --taxon bacteria --asf \\\n  --genefinding-tool prodigal \\\n  5.assembly_evaluation/all_assembled_genomes/scaffolds_${array[$SLURM_ARRAY_TASK_ID]}.m1000.fasta  \\\n  --output-dir 8.analyses/BGC/${array[$SLURM_ARRAY_TASK_ID]}\n\n\n\n\n\n\n\nExplanation of AntiSMASH parameters\n\n\n\n\n\n–cb-subclusters\ncompare identified clusters against known subclusters responsible for synthesising precursors\n\n\n–cb-knownclusters\ncompare identified clusters against known gene clusters from the MIBiG database\n\n\n–cb-general\ncompare identified clusters against a database of antiSMASH-predicted clusters\n\n\n–smcog-trees\ngenerate phylogenetic trees of sec. met. cluster orthologous groups.\n\n\n-c\nnumber of CPUs\n\n\n–taxon\ntaxonomic classification of input sequence\n\n\n–asf\nrun active site finder analysis\n\n\n–genefinding-tool\nspecify algorithm used for gene finding\n\n\n\nOutput files - index.html: web browser based summary - .gbk files are created for each biosynthetic gene cluster identified\n\n\n\n\nLet’s make a quick and easy phylogenetic tree with the 10 isolates, using the core gene alignment generated by GTDB-Tk and FastTree. FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences.\n1. Make a new directory\nmkdir -p 8.analyses/phylogenetic_tree\n2. Fetch the core gene alignment from GTDB-Tk\nUnzip and move over gtdbtk.bac120.user_msa.fasta.gz\ngzip -d 7.genome_annotation/taxonomy/align/gtdbtk.bac120.user_msa.fasta.gz &gt; 8.analyses/phylogenetic_tree/gtdbtk.bac120.user_msa.fasta\n\ncd 8.analyses/phylogenetic_tree/\n3. Build the phylogenetic tree\nIn the terminal, load the FastTree module then run it.\nmodule load FastTree/2.1.11-GCCcore-9.2.0\n\nFastTree gtdbtk.bac120.user_msa.fasta &gt; tree_file.txt\nWe will move to RStudio now to visualise the tree.",
    "crumbs": [
      "Day Two",
      "Analyses Visualisation"
    ]
  }
]